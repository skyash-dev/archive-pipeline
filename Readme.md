# ðŸ“¦ archive

A powerful pipeline to **extract, convert, and preserve webpages** into structured archives with:

- Cleaned metadata
- Markdown conversion
- Media extraction
- Outbound links
- WARC (Web ARChive) file generation
- Optional AI assistance via **Gemini LLM** for missing metadata

![Demo Screenshot](./archive-pipeline.png)

---

## âœ¨ Features

- ðŸ§  Auto-extract readable content via [Mozilla Readability](https://github.com/mozilla/readability)
- ðŸ“ Converts HTML content to Markdown using [Turndown](https://github.com/mixmark-io/turndown)
- ðŸ“¦ Saves full WARC archive using `wget`
- ðŸ”Ž Extracts all media and outbound links
- ðŸ¤– Optionally uses **Gemini AI** to fill missing metadata like author, date, description

---

## ðŸ“¥ Installation

```bash
npm install -g archive
```

---

## ðŸš€ Usage

```ts
import { archivePipeline } from "archive";

const result = await archivePipeline("https://example.com/article");

console.log(result.metadata); // title, author, date, dek, lead_image_url
console.log(result.markdown); // Markdown version of the page
console.log(result.media); // List of image and video URLs
console.log(result.links); // List of all hyperlinks
console.log(result.warcPath); // Path to saved .warc.gz file
```

---

## ðŸ“š Output Format

```ts
type ArchiveResult = {
  metadata: {
    title: string | null;
    author: string | null;
    date_published: string | null;
    dek: string | null;
    lead_image_url: string | null;
  };
  markdown: string;
  media: string[];
  links: string[];
  warcPath: string;
};
```

---

## ðŸ§  Gemini LLM Integration (Optional)

If fields like `author`, `date_published`, or `dek` are missing, the CLI will ask:

> "Would you like to use Gemini AI to infer missing metadata?"

If you agree, youâ€™ll be prompted to **safely enter your API key**, and the model will use the pageâ€™s title and content to generate richer metadata.

> âœ… Your key is never saved without consent.

---

## ðŸ”§ Dependencies

- `@mozilla/readability` â€“ extract clean article text
- `@postlight/parser` â€“ extract metadata using meta tags
- `jsdom` â€“ emulate browser environment
- `turndown` â€“ convert HTML to Markdown
- `wget` â€“ for saving full web archive (WARC)

---

## ðŸ“‚ Project Structure

```
src/
â”œâ”€â”€ archive/
â”‚   â”œâ”€â”€ index.ts          # Entry point
â”‚   â”œâ”€â”€ pipeline.ts       # Main logic pipeline
â”‚   â”œâ”€â”€ warc.ts           # WARC file generator
â”‚   â”œâ”€â”€ types.ts          # Output types
â”‚   â”œâ”€â”€ llm.ts            # Gemini LLM helper
â”‚   â””â”€â”€ askLLMConsent.ts  # Consent + key prompt
```

---

## ðŸ’¡ Example CLI Use

```bash
npm archive https://example.com
```

---

## ðŸ™Œ Contributing

PRs welcome! If youâ€™d like to contribute support for other LLMs, alternate archivers, or extra metadata, feel free to fork and submit.
